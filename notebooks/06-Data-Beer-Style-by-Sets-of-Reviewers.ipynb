{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick cell to make jupyter notebook use the full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from bokeh.plotting import show, save, output_notebook, output_file\n",
    "from bokeh.resources import INLINE \n",
    "output_notebook(resources=INLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import workflow\n",
    "from src.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load('beer_review_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ds.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning Beer Styles into Sets of Reviewers\n",
    "\n",
    "If we are going to embed beer styles by sets of reviewers then we need to turn our reviews data frame into a frame with one row per beer style instead of one row per review.\n",
    "\n",
    "This is a job for groupby.  We groupby the column we'd like to embedd and then use agg with a dictionary of column names to aggregation functions to tell it how to summarize the many reviews about a single beer into one record.  Aggregation functions are pretty much any function that takes an iterable and returns a single value.  Median and max are great functions for dealing with numeric fields.  First is handy for a field that you know to be common across for every beer review.  In other words fields that are tied to the beer such as brewery_name or beer_abv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to turn categorical data into a document of space seperated strings.  We want to do this to keep a nice easy pipeline for sklearns CountVectorizer.  A very natural way to accomplish this is via pandas df.groupby() function with a \" \".join(my_array) aggregator passed in.  Unfortunately, it turns out that \" \".join(my_array) seems to have trouble on for lists (or sequences) longer than 3,000 or so.  \n",
    "\n",
    "As such we've included a simple (though not necessarily efficient) join function that scales to large arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import custom_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(custom_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "unique_join = lambda x: custom_join(x.unique(), \" \")\n",
    "beer_style = reviews.groupby('beer_style').agg({\n",
    "    'beer_name':lambda x: x.mode(),\n",
    "    'brewery_name':lambda x: x.mode(),\n",
    "    'beer_abv':'mean',\n",
    "    'review_aroma':'mean',\n",
    "    'review_appearance':'mean',\n",
    "    'review_overall':'mean',\n",
    "    'review_palate':'mean',\n",
    "    'review_taste':'mean',\n",
    "    'review_profilename':[unique_join, len],\n",
    "    'brewery_id':lambda x: len(x.unique()),\n",
    "}).reset_index()\n",
    "\n",
    "beer_style.columns = \"\"\"beer_style beer_name brewery_name beer_abv \n",
    "review_aroma review_appearance review_overall review_palate review_taste \n",
    "review_profilename_list num_reviewers num_ids\"\"\".split()\n",
    "beer_style.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_style.head(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add this as a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_style_to_reviewers(review_dset):\n",
    "    \"\"\"\n",
    "    Turn our reviews data frame into a frame with one row per beer style instead of one row per review.\n",
    "\n",
    "    We groupby the column we'd like to embedd and then use agg with a dictionary of column names to \n",
    "    aggregation functions to tell it how to summarize the many reviews about a single beer into one record.\n",
    "    (Median and max are great functions for dealing with numeric fields).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    review_dset: Dataset\n",
    "        Dataset containing the beer reviews data\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    beer style dataset with a dataframe representing beer style by reviewers\n",
    "    \"\"\"\n",
    "    reviews = review_dset.data\n",
    "    unique_join = lambda x: custom_join(x.unique(), \" \")\n",
    "    beer_style = reviews.groupby('beer_style').agg({\n",
    "        'beer_name':lambda x: x.mode(),\n",
    "        'brewery_name':lambda x: x.mode(),\n",
    "        'beer_abv':'mean',\n",
    "        'review_aroma':'mean',\n",
    "        'review_appearance':'mean',\n",
    "        'review_overall':'mean',\n",
    "        'review_palate':'mean',\n",
    "        'review_taste':'mean',\n",
    "        'review_profilename':[unique_join, len],\n",
    "        'brewery_id':lambda x: len(x.unique()),\n",
    "    }).reset_index()\n",
    "\n",
    "    beer_style.columns = \"\"\"beer_style beer_name brewery_name beer_abv \n",
    "    review_aroma review_appearance review_overall review_palate review_taste \n",
    "    review_profilename_list num_reviewers num_ids\"\"\".split()\n",
    "    ds_reviewers = Dataset(dataset_name=\"beer_style_reviewers\", metadata=review_dset.metadata, data=beer_style)\n",
    "    return ds_reviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.transformers import groupby_style_to_reviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = groupby_style_to_reviewers(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_style = new_ds.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now add it to `transformers.py`. And the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations=[\n",
    "    (\"groupby_style_to_reviewers\", {}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_transformer(input_dataset='beer_review_all', transformations=transformations, output_dataset=\"beer_style_reviewers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.make_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform by vectorizing the sets of reviewers (embedding strings into a vector space)\n",
    "\n",
    "We are going to vectorize our data and look at the number of categorical values they have in common.  A useful thing to do here is to require each row to have a minimum support before being included.  Filtering this early, will ensure indices line up later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_style.review_profilename_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This step turns a sequence of space seperated text into a sparse matrix of counts.  One row per row of our data frame and one column per unique token that appeared in our categorical field of interest.\n",
    "\n",
    "If we want to deal with sets (i.e. just presence or absence of a category) use:<BR>\n",
    "`beer_by_authors_vectorizer = CountVectorizer(binary=True)`<BR>\n",
    "If we think counts should matter we might use:<BR>\n",
    "`beer_by_authors_vectorizer = CountVectorizer()`<BR>\n",
    "or if we want to correct for very unbalanced column frequencies:<BR>\n",
    "`beer_by_authors_vectorizer = TfidfVectorizer()`<BR>\n",
    "    \n",
    "We use `min_df=10` in our CountVectorize to only count reviewers who have reviewed at least 10 beers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "style_by_authors_vectorizer = CountVectorizer(binary=True, min_df=10)\n",
    "style_by_authors = style_by_authors_vectorizer.fit_transform(beer_style.review_profilename_list)\n",
    "style_by_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's add it to the workflow\n",
    "\n",
    "Instead of creating individual transfomers for each sklearn transformer (which we can do), \n",
    "let's use a generic wrapper for applying sklearn transformers to Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.transformers import sklearn_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sklearn_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we're doing the same thing\n",
    "\n",
    "cv_opts = {\"binary\":True, \"min_df\":10}\n",
    "transformer_opts = {\n",
    "    \"transformer_name\": \"CountVectorizer\",\n",
    "    \"subselect_column\":'review_profilename_list',\n",
    "    \"transformer_opts\":cv_opts\n",
    "}\n",
    "style_by_authors_dset = sklearn_transform(new_ds, **transformer_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add this transformer to the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_transformers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations=[\n",
    "    (\"sklearn_transform\", transformer_opts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_transformer(input_dataset='beer_style_reviewers', transformations=transformations, output_dataset='style_by_authors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_by_authors = Dataset.load(\"style_by_authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_by_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_style = Dataset.load(\"beer_style_reviewers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_style.data.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to complete our embedding. \n",
    "\n",
    "### XXX\n",
    "In retrospect, I probably should have made CountVectorizer part of the embedding pipeline. Next notebook I'll try it that way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reproallthethings] *",
   "language": "python",
   "name": "conda-env-reproallthethings-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
