{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick cell to make jupyter notebook use the full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Enable autoreloading from src\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from bokeh.plotting import show, save, output_notebook, output_file\n",
    "from bokeh.resources import INLINE \n",
    "output_notebook(resources=INLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from src.data.numba_word_vectorizer import word_word_cooccurence_matrix\n",
    "from src.data.em_method import em_sparse\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "from enstop import PLSA\n",
    "import umap\n",
    "import umap.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import workflow\n",
    "from src.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data\n",
    "\n",
    "In this case we're going to be doing a joint word-document embedding. All we need are the reviews as a list of separate documents to start with.\n",
    "\n",
    "This part is from https://github.com/lmcinnes/umap/blob/master/notebooks/Document%20embedding%20using%20UMAP.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = fetch_20newsgroups(subset='test',\n",
    "                             shuffle=True,\n",
    "                             random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(dataset.data)} documents')\n",
    "print(f'{len(dataset.target_names)} categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here are the categories of documents. As you can see many are related to one another (e.g. 'comp.sys.ibm.pc.hardware' and 'comp.sys.mac.hardware') but they are not all correlated (e.g. 'sci.med' and 'rec.sport.baseball').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple sample documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, document in enumerate(dataset.data[:3]):\n",
    "    category = dataset.target_names[dataset.target[idx]]\n",
    "    \n",
    "    print(f'Category: {category}')\n",
    "    print('---------------------------')\n",
    "    # Print the first 500 characters of the post\n",
    "    print(document[:500])\n",
    "    print('---------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab reviews that are long enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = pd.DataFrame(dataset.data, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need this. Everything has at least len 100\n",
    "raw_text = np.unique(np.array(raw_text[raw_text.text.str.len() > 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXX parse posts into their constituent parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Document Matrix\n",
    "\n",
    "We will deal with documents, in this case, newsgroup posts, as follows:\n",
    "\n",
    "A post is a multinomial distribution over our vocabulary. \n",
    "\n",
    "Step-by-step that means:\n",
    "* A post is a bag of words\n",
    "* TfidfVectorizer -> bag of words -> bag of normalized multinomial distributions over the vocabulary (i.e. weighted multinomials)\n",
    "    * If we had used CountVectorizer we would have a bag of multinomial distributions\n",
    "* Use Expectation-Maximization (EM) to remove the average from the matrix (think of it like projecting away from the global trends of language coming from grammar and common word usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_matrix, token_to_index, index_to_token = word_word_cooccurence_matrix(raw_text, min_df=50)\n",
    "raw_doc_matrix = TfidfVectorizer(vocabulary=token_to_index, norm='l1').fit_transform(raw_text)\n",
    "raw_doc_matrix.eliminate_zeros()\n",
    "print(raw_doc_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With remove expectation:\n",
    "\n",
    "    (18846, 8809)\n",
    "    CPU times: user 40.1 s, sys: 969 ms, total: 41.1 s\n",
    "    Wall time: 41.5 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove zero rows - docs that don't have any common words\n",
    "is_nonempty_row = np.array(raw_doc_matrix.sum(axis=1).T)[0] != 0\n",
    "text = raw_text[is_nonempty_row]\n",
    "doc_matrix = raw_doc_matrix[is_nonempty_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global parameters for the joint embedding\n",
    "\n",
    "`background_prior`: \n",
    "This is a positive number, 1 being neutral, <1 underweight and >1 overweight wrt the strength of the background. Higher will tend to make things more orthogonal, and will cluster things more tightly (in theory) at the expense of global structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_prior = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`joint_dimension`: We will later learn a word embedding into this dimension with PLSA and then map the documents as an average of word vectors. The higher the better for accuracy, but it will be slower and more memory intensive. 300 is the word2vec range, so we started with that and it seemed good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_dimension = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do EM on the Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D, mix_params = em_sparse(TfidfTransformer(norm='l1').fit_transform(doc_matrix), prior_noise=background_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the upshots of using EM is that our matrix is sparser now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of non-zero entries before EM: {raw_doc_matrix.nnz}')\n",
    "print(f'Number of non-zero entries after EM:   {D.nnz}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Word Matrix\n",
    "\n",
    "Since we're doing a joint embedding, we will treat each word like a document of its context (before it and after it which we treat separately), and then embed the words in the same way that we did the documents.\n",
    "\n",
    "More precisely, think of a word as a document of \"contexts containing that word\"; that is, of two sets of documents, the context windowns before the word, and the context window after the word. We treat a word as two documents, and do exactly as we did above for each document (aka. set of context windows), concatinating the result into a vector of length 2 times the size of the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "W, s_w = em_sparse(TfidfTransformer(norm='l1').fit_transform(word_matrix), prior_noise=background_prior)\n",
    "Wt, s_wt = em_sparse(TfidfTransformer(norm='l1').fit_transform(word_matrix.T), prior_noise=background_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_mat_directed = normalize(scipy.sparse.hstack([W, Wt]), norm='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give the Word Matrix and Doc Matrix the same basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because words are related (not independent), we don't want to think of a document as the average of the 1-hot encoded vectory corresponding to each word. Instead, we want to change basis so that we can consider a document as a weighted linear combination of the word vectors. If we do this naively, we'll end up with a huge dense matrix.\n",
    "\n",
    "Instead, let's dimension reduce the word vectors, so that we're considering a document as a weighted linear combination of word vector topics. We'll use pLSA for this. Why? It is a linear dimension reduction technique for topic modelling that takes a bag of multinomials to a bag of multinomials. This is what we want. The dimension we reduce to will be the number of latent word-topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicer = PLSA(n_components=joint_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "topicer.fit(word_mat_directed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_by_topic = topicer.embedding_\n",
    "D_low_temp = D * word_by_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our matrix `D_low_temp` is now a dense ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_low_temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now averaged a bunch of things together again, and we have a central limit effect. We need to separate things away from the mean again. EM to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_low, s_list = em_sparse(scipy.sparse.csr_matrix(D_low_temp), prior_noise=background_prior)\n",
    "D_low = D_low.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the Joint Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_docs = D_low.shape[0]\n",
    "n_docs = 2000 #died on me with the full amount\n",
    "\n",
    "## use sqrt and euclidean distance instead of Hellinger\n",
    "w_and_d = np.sqrt(np.vstack((word_by_topic, D_low[:n_docs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_and_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXX replace this with hellinger later, check that I get the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set up some labels to use for hovering over our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_labels = [dataset.target_names[x] for x in dataset.target]\n",
    "hover_df = pd.DataFrame(category_labels, columns=['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_top_words(row):\n",
    "    inds = row.indices\n",
    "    data = row.data\n",
    "    order = np.argsort(-data)\n",
    "    return inds[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_indices = [doc_top_words(doc_matrix.getrow(i)) for i in range(doc_matrix.shape[0])]\n",
    "supported_words_array = np.array([\" \".join([index_to_token[index_list[i]] for i in range(min(10, len(index_list)))]) for index_list in col_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_array = np.array([index_to_token[x] for x in range(W.shape[0])])\n",
    "wd_labels = np.hstack((np.zeros(word_by_topic.shape[0]), np.ones(n_docs)))\n",
    "wd_hover_df = pd.DataFrame({'text': np.hstack([word_array, supported_words_array])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, use UMAP to embed the words and docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mapping = umap.UMAP(n_neighbors=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding = mapping.fit(w_and_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = umap.plot.interactive(embedding, hover_data=wd_hover_df, labels=wd_labels, width=800, height=800, point_size=5);\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reproallthethings] *",
   "language": "python",
   "name": "conda-env-reproallthethings-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
